In this paper, we present CacheInf, a collaborative edge-cloud cache system for efficient robotic visual model inference.
Based on the continuity of visual input of robots in the field and the local operators commonly used in visual models, we introduce cache mechanism to visual model inference in CacheInf.
By reusing computation results of similar local geometries between consecutive inputs, CacheInf accelerates visual model inference by reducing both local computation time and transmission time when offloading computation to the GPU server.
The more real-time visual model inference on robots enabled by CacheInf will nurture more visual models to be deployed in real-world robots.