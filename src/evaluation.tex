\subsection{Evaluation Settings}

\begin{figure}[!t]
    \centering
    \subfloat[Four-wheeled robot]{\includegraphics[width=0.48\linewidth]{fig/robot.png}\label{fig:robot}}
    \hfil
    \subfloat[Air-ground robot]{\includegraphics[width=0.48\linewidth]{fig/agr_robot.png}\label{fig:agr}}
    \caption{The composition of the four-wheeled robot and the air-ground robot used in our evaluation.}
    % \label{fig:robot}
\end{figure}

\myparagraph{Testbed}
We conducted experiments on a four-wheeled robot (Fig~\ref{fig:robot}) and a air-ground robot(Fig~\ref{fig:agr}).
Both robots are equipped with a Jetson Xavier NX~\cite{jetsonnx} 8G onboard computer with cuda acceleration capability and a MediaTek MT76x2U USB wireless network interface card for wireless connectivity.
The Jetson Xavier NX is connected to a Leishen N10P LiDAR, an ORBBEC Astra depth camera and an STM32F407VET6 controller via USB serial ports, which are managed and driven using ROS Noetic. 
The GPU server used in our experiments is equipped with an Intel(R) i5 12400f CPU @ 4.40GHz and an NVIDIA GeForce GTX 2080 Ti 11GB GPU, connected to our robot via Wi-Fi 6 over 80MHz channel at 5GHz frequency.

% Tab.~\ref{tab:energydefault} presents the overall on-board energy consumption (excluding motor energy consumption for robot movement) of the robot in various states: inference (model inference with full GPU utilization, including CPU and GPU energy consumption), transmission (communication with the GPU server, including wireless network card energy consumption), and standby (robot has no tasks to execute).
% Notice that different models, due to varying numbers of parameters, exhibit distinct GPU utilization rates and power consumption during inference. 

% \begin{table}[!t]
%     \centering
%     \begin{tabular}{|c|c|c|c|}
%     \hline
%             & inference & transmission & standby \\ \hline
%     Power (W) &     13.35        &       4.25        &    4.04   \\ \hline
%     \end{tabular}
%     \caption{Power consumption (Watt) of our robot in different states.}
%     \label{tab:energydefault}
%     \end{table}

\myparagraph{Workload}
We chose two real-world visual robotic applications as our major workloads: 1. Kapao~\cite{kapao} depicted in Figure~\ref{fig:kapao}, a RGB-image-based real-time people key point detection applications used to guide our four-wheeled robot to track and follow a walking people;
2. AGRNav~\cite{agrnav} depicted in Figure~\ref{fig:agr}, an autonomous air-ground robot navigation application that predicts unobserved obstacles by semantic prediction on point clouds and optimizes the navigation trajectory for the air-ground robot.
% We evaluated two typical real-world robotic applications on our testbed: Kapao, a real-time people-tracking application on our four-wheeled robot (Fig~\ref{fig:kapao}), and AGRNav, an autonomous navigation application on our air-ground robot (Fig~\ref{fig:agrnav}). 
% These applications feature different model input and output size patterns: Kapao takes RGB images as input and outputs key points of small data volume. In contrast, AGRNav takes point clouds as input and outputs predicted point clouds and semantics of similar data volume as input, implying that AGRNav needs to transmit more data during distributed inference. 
We also verified CacheInf's performance on a broader range of visual models common to mobile devices: VGGNet~\cite{simonyan2015deep}, ConvNeXt~\cite{woo2023convnext}, RegNet~\cite{xu2022regnet}, and we used their default implementation of torchvision~\cite{noauthor_torchvision_nodate}. 
% And we have verified several models common to mobile devices on a larger scale to further corroborate our observations and findings: DenseNet~\cite{huang2018densely}, VGGNet~\cite{simonyan2015deep}, ConvNeXt~\cite{woo2023convnext}, RegNet~\cite{xu2022regnet}.
% The models' running statistics are listed in Tab.~\ref{tab:all_app}.

\myparagraph{Dataset}
For AGRNav we used the officially available sequence of point clouds input~\cite{agrnav} and for Kapao, we used the Collective Activity Dataset (CAD)~\cite{Choi_VSWS_2009} which are sequences of video images of people doing different activities captured using hand-held cameras.
For the rest of the models from torchvision, we used the DAVIS~\cite{Perazzi2016} dataset which are sequences of video images of different objectives captured also using hand-held cameras.

\myparagraph{Experiment Environments}
The experiments across all systems and all workloads were conducted in two different real-world environments: 1. indoors, where the robot was moving in our office with desks and separators interfering wireless bandwidth;
2. outdoors, where the robot was moving in a garden with trees and bushes interfering with wireless signals and less reflection, resulting in lower bandwidth. 
The bandwidth fluctuation of each of the environments are shown in Fig.~\ref{fig:bandwidth}.
% We evaluated two real-world environments: indoors (robots move in our laboratory with desks and separators interfering with wireless signals) and outdoors (robots move in our campus garden with trees and bushes interfering with wireless signals, resulting in lower bandwidth). 

\myparagraph{Baselines}
We selected two SOTA inference acceleration methods as baselines: DSCCS ~\cite{liang2023dnn}, which searches for optimal layer partition strategy of a visual model to offload layers to the GPU server to accelerate inference, and Hybrid-Parallel ~\cite{sun2024hybridparallel} (referred to as HP), which enables parallelization of local computation and offloading by also partitioning the layer input / output of local operators besides layer partitioning to further accelerate inference. 
We also combined DSCCS with our cache mechanism (referred to as DSCCS-C) to present another perspective about CacheInf's performance gain.

The evaluation questions are as follows:
\begin{itemize}
    \item RQ1: How much does CacheInf benefit real-world robotic applications by reducing inference time and energy consumption?
    \item RQ2: How does CacheInf perform on more models common to mobile devices?
    \item RQ3: How is the above gain achieved in CacheInf and what affects it?
    \item RQ4: What are the limitations and potentials of CacheInf?
\end{itemize}

\begin{figure}[!t]
    \centering
    \subfloat[Targeted people]{\includegraphics[width=0.95\linewidth]{fig/people.drawio.png}}
    \vfil
    \subfloat[Robot moving trajectory]{\includegraphics[width=0.95\linewidth]{fig/robot.drawio.png}}
    \caption{A real-time people-tracking robotic application on our robot based on a state-of-the-art human pose estimation visual model, Kapao~\cite{kapao}.}
    \label{fig:kapao}
\end{figure}

\begin{figure}[!t]
    \centering
    \subfloat[Indoors]{\includegraphics[width=0.8\linewidth]{fig/agrnav.png}}

    \subfloat[Outdoors]{\includegraphics[width=0.8\linewidth]{fig/agr_outdoors.png}}
    \caption{By predicting occlusions in advance, AGRNav~\cite{agrnav} gains an accurate perception of the environment and avoids collisions, resulting in efficient and energy-saving paths.}
    \label{fig:agrnav}
\end{figure}


\subsection{End-to-End Performance on Real-World Applications}

\begin{table*}[htb]
    \centering
\begin{tabular}{ccc|c|c|c|c|c|c}
\toprule
Model(number & Local compu- & \multirow[c]{2}{*}{System} & \multicolumn{2}{|c|}{Transmission time/s} & \multicolumn{2}{|c|}{Inference time/s} & \multicolumn{2}{c}{Percentage(\%)} \\
of parameters)& tation time/s &  & indoors & outdoors & indoors & outdoors & indoors & outdoors \\
\midrule
\multirow[c]{4}{*}{Kapao(77M)} & \multirow[c]{4}{*}{1.01($\pm$0.03)} & DSCCS & 0.21($\pm$0.1) & 0.24($\pm$0.12) & 0.36($\pm$0.2) & 0.40($\pm$0.17) & 58.33 & 60.21 \\
 &  & DSCCS-C & 0.22($\pm$0.14) & 0.25($\pm$0.12) & 0.32($\pm$0.25) & 0.34($\pm$0.18) & 68.75 & 73.53 \\
 &  & HP & 0.24($\pm$0.15) & 0.28($\pm$0.13) & 0.31($\pm$0.14) & 0.34($\pm$0.12) & 77.42 & 82.35 \\
 &  & CacheInf & 0.16($\pm$0.13) & 0.21($\pm$0.18) & 0.20($\pm$0.16) & 0.24($\pm$0.20) & 80.09 & 87.56 \\
\cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{AGRNav(0.84M)} & \multirow[c]{4}{*}{0.60($\pm$0.04)} & DSCCS & 0.10($\pm$0.05) & 0.15($\pm$0.05) & 0.41($\pm$0.11) & 0.47($\pm$0.12) & 24.39 & 31.91\\
 &  & DSCCS-C & 0.13($\pm$0.07) & 0.16($\pm$0.06) & 0.38($\pm$0.10) & 0.43($\pm$0.13) & 34.21 & 37.21\\
 &  & HP & 0.24($\pm$0.08) & 0.26($\pm$0.07) & 0.30($\pm$0.09) & 0.33($\pm$0.07) & 78.65 & 79.47 \\
 &  & CacheInf & 0.18($\pm$0.08) & 0.20($\pm$0.08) & 0.21($\pm$0.16) & 0.25($\pm$0.18) & 86.71 & 80.01 \\
\cline{1-9} \cline{2-9}
\bottomrule
\end{tabular}

    \caption{Average transmission time, inference time, percentage that transmission time accounts for of the total inference time and their standard deviation ($\pm n$) of Kapao and AGRNav in different environments with different systems. ``Local computation'' refers to inference the entire model locally on the robot.}
    \label{tab:e2e_time}
\end{table*}

Table~\ref{tab:e2e_time} shows the end-to-end inference latency and the ratio that transmission time takes up the inference latency and compared with the baselines, CacheInf reduced inference latency by 35.5\% to 44.4\% indoors and 29.4\% to 40.0\% outdoors for Kapao and 30.0\% to 48.8\% indoors and 24.2\% to 46.8\% outdoors for AGRNav.
Compared with HP, while CacheInf reduced transmission time by 6 to 8 ms, CacheInf further reduced inference latency by 8 to 10 ms, confirming the effectiveness of the acceleration of the used sparse local operators.
CacheInf's highest percentage that transmission time takes up the inference latency across all cases shows that with shrunk transmission data volume with cache enabled and the integration of HP, CacheInf tend to offload computation to the GPU server more often.
This can also be validated by the increased transmission time and shortened inference latency of DSCCS-C compared with DSCCS.

The evidently reduced inference latency of CacheInf leads to reduction of energy consumed per inference by 25.2\% to 34.3\% indoors and 21.2\% to 34.0\% outdoors for Kapao and 27.4\% to 35.7\% indoors and 21.7\% to 39.9\% outdoors for AGRNav, as shown in Table~\ref{tab:e2e_power}, while the runtime power consumption was increased due to higher frequency of inference.

We report the peak GPU memory consumption on the robot under different strategy in Table~\ref{tab:e2e_mem}: CacheInf (includes both the systems of CacheInf and DSCCS-C), No Cache (includes local computation and HP) and Cache All (a naive strategy that caches the output of every layer).
The results show that CacheInf increased peak GPU memory consumption by 64.6\% for Kapao and 58.5\% for AGRNav compared with no cache, which is however 72.2\% and 81.6\% lower than the cases of Cache All, demonstrating the effectiveness of CacheInf's strategy to reduce the number cached operators.


\begin{table*}[htb]
\centering
\begin{tabular}{cc|c|c|c|c}
\toprule
 Model(number & \multirow[c]{2}{*}{System} & \multicolumn{2}{|c|}{Power consumption(W)} & \multicolumn{2}{|c}{Energy consumption(J) per inference} \\
 of parameters)&  & indoors & outdoors & indoors & outdoors \\
\midrule
\midrule
\multirow[c]{5}{*}{Kapao(77M)} & Local & 10.61($\pm$0.49) & 10.61($\pm$0.49) & 9.79($\pm$0.03) & 9.79($\pm$0.03) \\
 & DSCCS & 6.38($\pm$2.21) & 6.63($\pm$2.38) & 2.30($\pm$0.55) & 2.65($\pm$0.55) \\
 & DSCCS-C & 6.30($\pm$2.15) & 6.53($\pm$2.12) & 2.02($\pm$0.50) & 2.22($\pm$0.53) \\
 & HP & 7.05($\pm$1.63) & 6.94($\pm$0.98) & 2.19($\pm$0.62) & 2.35($\pm$0.42) \\
 & CacheInf & 7.53($\pm$1.62) & 7.30($\pm$0.96) & 1.51($\pm$0.60) & 1.75($\pm$0.41) \\
\cline{1-6}
\multirow[c]{5}{*}{AGRNav(0.84M)} & Local & 8.11($\pm$0.25) & 8.11($\pm$0.25) & 4.86($\pm$0.01) & 4.86($\pm$0.01) \\
 & DSCCS & 6.21($\pm$1.50) & 7.29($\pm$1.55) & 2.55($\pm$0.19) & 3.43($\pm$0.18) \\
 & DSCCS-C & 6.17($\pm$1.56) & 7.00($\pm$1.43) & 2.34($\pm$0.20) & 3.01($\pm$0.20) \\
 & HP & 7.52($\pm$0.51) & 8.04($\pm$0.45) & 2.26($\pm$0.15) & 2.63($\pm$0.15) \\
 & CacheInf & 7.83($\pm$0.57) & 8.23($\pm$0.56) & 1.64($\pm$0.17) & 2.06($\pm$0.16) \\
\cline{1-6}
\bottomrule
\end{tabular}

    \caption{The power consumption against time (Watt) and energy consumption per inference (Joule) with standard deviation ($\pm n$) of Kapao and AGRNav different environments with different systems. ``Local'' represents ``Local computation''.}
    \label{tab:e2e_power}
\end{table*}

\begin{table}[htb]
\centering
\begin{tabular}{c|c|c|c}
\toprule
Model(number & \multicolumn{3}{|c}{Memory Consumption(MB)} \\
 of parameters) & No Cache & Cache All & CacheInf \\
\midrule
Kapao(77M) & 300.6 & 1782.5 & 494.7\\
\hline
AGRNav(0.84M) & 82.8 & 713.3 & 131.2\\
\bottomrule
\end{tabular}

    \caption{Peak GPU memory consumption of different caching strategy on Kapao and AGRNav. }
    \label{tab:e2e_mem}
\end{table}

\subsection{Performance on Various Common Models}
The above conclusions can be further validated by results of a wider range of visual models in Table~\ref{tab:torchvision_time} and Table~\ref{tab:torchvision_power}.
Across different visual models, CacheInf reduced the inference latency by 13.4\% to 43.6\% indoors and 13.1\% to 45.9\% outdoors, and it results in the reduction in energy consumed per inference to be 11.1\% to 46.7\% indoors and 9.5\% to 42.2\%
compared with the baselines.
Note that although CacheInf's gain is still evident, the lower bound of CacheInf's gain decreased on these models compared with Kapao and AGRNav; the reason could be that these models are less computation-intensive, which can be implied from their shorter time for local computation compared with Kapao and AGRNav.
When inference of a visual models is not computation-intensive, the gain of using sparse local operators in CacheInf will be limited since execution of each local operator will no longer be the bottleneck.
In terms of GPU memory consumption, CacheInf increased GPU memory consumption by 3.2\% to 24.8\% compared with No Cache, while reducing 12.8\% to 39.5\% GPU memory consumption compared with Cache All.

\begin{table*}[htb]

    \centering
\begin{tabular}{ccc|c|c|c|c|c|c}
\toprule
 Model(number&  Local compu- & \multirow[c]{2}{*}{System} & \multicolumn{2}{|c|}{Transmission time/ms} & \multicolumn{2}{|c|}{Inference time/ms} & \multicolumn{2}{c}{Percentage(\%)} \\
 of parameters) & taion time/ms &  & indoors & outdoors & indoors & outdoors & indoors & outdoors \\
\midrule
% \multirow[c]{4}{*}{DenseNet(7M)} & \multirow[c]{4}{*}{74.5($\pm$18.7)} & DSCCS & 16.2($\pm$40.9) & 20.8($\pm$51.9) & 81.4($\pm$27.2) & 86.6($\pm$27.7) & 19.95 & 24.07 \\
%  &  & DSCCS-C & 20.4($\pm$43.5) & 25.8($\pm$56.9) & 85.5($\pm$27.9) & 89.6($\pm$29.3) & 23.86 & 28.80 \\
%  &  & HP & 53.4($\pm$34.5) & 52.9($\pm$23.9) & 74.5($\pm$85.7) & 55.1($\pm$15.6) & 71.70 & 96.05 \\
%  &  & CacheInf & 56.3($\pm$37.5) & 57.5($\pm$43.5) & 76.3($\pm$90.6) & 78.1($\pm$33.6) & 73.79 & 73.62 \\
% \cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{RegNet(54M)} & \multirow[c]{4}{*}{175.0($\pm$23.6)} & DSCCS & 47.6($\pm$47.8) & 60.5($\pm$54.0) & 77.8($\pm$39.3) & 86.2($\pm$37.9) & 61.22 & 70.22 \\
 &  & DSCCS-C & 50.7($\pm$49.8) & 62.5($\pm$53.6) & 70.8($\pm$33.3) & 79.5($\pm$39.2) & 71.61 & 78.61 \\
 &  & HP & 49.6($\pm$21.7) & 59.9($\pm$23.4) & 55.0($\pm$24.8) & 64.2($\pm$25.2) & 90.18 & 93.34 \\
 &  & CacheInf & 44.2($\pm$27.7) & 48.5($\pm$25.3) & 45.3($\pm$35.0) & 49.2($\pm$37.2) & 97.57 & 98.58 \\
\cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{ConvNeXt(88M)} & \multirow[c]{4}{*}{160.2($\pm$21.0)} & DSCCS & 46.9($\pm$43.1) & 56.7($\pm$52.1) & 72.4($\pm$35.7) & 84.7($\pm$36.3) & 64.78 & 66.95 \\
 &  & DSCCS-C & 48.0($\pm$45.0) & 53.2($\pm$50.1) & 56.8($\pm$28.1) & 70.8($\pm$39.0) & 84.51 & 75.14 \\
 &  & HP & 50.4($\pm$32.2) & 61.9($\pm$34.8) & 53.9($\pm$26.2) & 65.7($\pm$27.7) & 93.51 & 94.23 \\
 &  & CacheInf & 40.7($\pm$40.0) & 50.7($\pm$40.3) & 46.7($\pm$35.4) & 56.8($\pm$45.0) & 87.15 & 89.26 \\
\cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{VGG19(143M)} & \multirow[c]{4}{*}{118.0($\pm$18.9)} & DSCCS & 38.9($\pm$47.1) & 41.6($\pm$53.8) & 65.2($\pm$28.1) & 75.5($\pm$27.1) & 59.75 & 55.09 \\
 &  & DSCCS-C & 42.7($\pm$30.2) & 52.0($\pm$50.3) & 53.2($\pm$33.0) & 60.3($\pm$30.9) & 80.26 & 86.24 \\
 &  & HP & 44.8($\pm$20.9) & 51.5($\pm$15.0) & 47.6($\pm$18.1) & 53.6($\pm$14.7) & 94.15 & 96.07 \\
 &  & CacheInf & 37.8($\pm$31.2) & 43.5($\pm$13.2) & 41.1($\pm$20.3) & 46.6($\pm$12.8) & 94.26 & 93.34 \\
\cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{ConvNeXt(197M)} & \multirow[c]{4}{*}{316.7($\pm$31.0)} & DSCCS & 56.0($\pm$36.1) & 67.0($\pm$37.6) & 79.2($\pm$35.9) & 90.6($\pm$35.4) & 70.72 & 73.98 \\
 &  & DSCCS-C & 56.0($\pm$39.0) & 63.0($\pm$30.2) & 64.7($\pm$40.2) & 68.6($\pm$35.0) & 86.55 & 91.84 \\
 &  & HP & 56.4($\pm$34.7) & 66.5($\pm$33.7) & 59.7($\pm$26.6) & 68.0($\pm$26.6) & 94.43 & 97.88 \\
 &  & CacheInf & 40.4($\pm$37.8) & 46.9($\pm$40.0) & 44.7($\pm$33.3) & 49.0($\pm$30.8) & 90.38 & 95.71 \\
\cline{1-9} \cline{2-9}
\bottomrule
\end{tabular}
    \caption{Average transmission time, inference time, percentage that transmission time accounts for of the total inference time and their standard deviation ($\pm n$) of common visual models in different environments with different systems. }
    \label{tab:torchvision_time}
\end{table*}

\begin{table*}[htb]

\centering
\begin{tabular}{cc|c|c|c|c}
\toprule
Model(number & \multirow[c]{2}{*}{System} & \multicolumn{2}{|c|}{Power consumption(W)} & \multicolumn{2}{|c}{Energy consumption(J) per inference} \\
of parameters)&  & indoors & outdoors & indoors & outdoors \\
\midrule
% \multirow[c]{5}{*}{DenseNet(7M)} & Local & 8.2($\pm$0.27) & 8.2($\pm$0.27) & 0.46($\pm$0.04) & 0.46($\pm$0.04) \\
%  & DSCCS & 6.91($\pm$0.45) & 6.86($\pm$0.46) & 0.56($\pm$0.04) & 0.59($\pm$0.04) \\
%  & DSCCS-C & 7.01($\pm$0.43) & 6.96($\pm$0.43) & 0.60($\pm$0.07) & 0.52($\pm$0.06) \\
%  & HP & 5.36($\pm$0.79) & 5.79($\pm$0.24) & 0.4($\pm$0.06) & 0.32($\pm$0.01) \\
%  & CacheInf & 6.01($\pm$0.92) & 6.31($\pm$0.56) & 0.46($\pm$0.12) & 0.49($\pm$0.01) \\
% \cline{1-6}
\multirow[c]{5}{*}{RegNet(54M)} & Local & 9.0($\pm$0.3) & 9.0($\pm$0.3) & 1.37($\pm$0.02) & 1.37($\pm$0.02) \\
& DSCCS & 5.84($\pm$1.79) & 5.36($\pm$1.34) & 0.45($\pm$0.14) & 0.46($\pm$0.12) \\
 & DSCCS-C & 6.04($\pm$1.88) & 5.96($\pm$1.45) & 0.43($\pm$0.16) & 0.47($\pm$0.19) \\
 & HP & 5.24($\pm$1.43) & 5.28($\pm$1.52) & 0.29($\pm$0.08) & 0.34($\pm$0.1) \\
 & CacheInf & 5.20($\pm$1.51) & 5.23($\pm$1.77) & 0.24($\pm$0.08) & 0.26($\pm$0.09) \\
\cline{1-6}
\multirow[c]{5}{*}{ConvNeXt(88M)} & Local & 9.7($\pm$0.34) & 9.7($\pm$0.34) & 1.34($\pm$0.02) & 1.34($\pm$0.02) \\
& DSCCS & 6.01($\pm$0.27) & 5.71($\pm$1.56) & 0.43($\pm$0.05) & 0.48($\pm$0.13) \\
& DSCCS-C & 6.20($\pm$0.33) & 5.91($\pm$0.21) & 0.35($\pm$0.17) & 0.42($\pm$0.25) \\
 & HP & 6.68($\pm$1.23) & 6.68($\pm$1.21) & 0.36($\pm$0.07) & 0.44($\pm$0.08) \\
 & CacheInf & 6.70($\pm$0.55) & 6.63($\pm$0.26) & 0.31($\pm$0.07) & 0.38($\pm$0.08) \\
\cline{1-6}
\multirow[c]{5}{*}{VGG19(143M)} & Local & 9.78($\pm$0.34) & 9.78($\pm$0.34) & 0.95($\pm$0.02) & 0.95($\pm$0.02) \\
& DSCCS & 6.58($\pm$2.14) & 6.93($\pm$2.35) & 0.43($\pm$0.14) & 0.52($\pm$0.18) \\
 & DSCCS-C & 6.82($\pm$2.10) & 7.23($\pm$2.45) & 0.36($\pm$0.18) & 0.43($\pm$0.30) \\
 & HP & 6.51($\pm$1.74) & 7.32($\pm$1.52) & 0.31($\pm$0.08) & 0.39($\pm$0.08) \\
 & CacheInf & 6.70($\pm$1.88) & 7.22($\pm$1.36) & 0.27($\pm$0.10) & 0.34($\pm$0.09) \\
\cline{1-6}
\multirow[c]{5}{*}{ConvNeXt(197M)} & Local & 10.72($\pm$0.38) & 10.72($\pm$0.38) & 3.12($\pm$0.03) & 3.12($\pm$0.03) \\
& DSCCS & 5.06($\pm$0.31) & 5.02($\pm$0.37) & 0.4($\pm$0.02) & 0.45($\pm$0.03) \\
 & DSCCS-C & 4.86($\pm$0.44) & 4.99($\pm$0.39) & 0.31($\pm$0.05) & 0.34($\pm$0.09) \\
 & HP & 4.57($\pm$0.23) & 4.54($\pm$0.25) & 0.27($\pm$0.01) & 0.31($\pm$0.02) \\
 & CacheInf & 5.26($\pm$0.40) & 5.39($\pm$0.27) & 0.24($\pm$0.05) & 0.26($\pm$0.04) \\
\cline{1-6}
\bottomrule
\end{tabular}
    \caption{The power consumption against time (Watt) and energy consumption per inference (Joule) with standard deviation ($\pm n$) of common visual models in different environments with different systems. ``Local'' represents ``Local computation''.}
    \label{tab:torchvision_power}
\end{table*}

\begin{table}[htb]
    \centering
    \begin{tabular}{c|c|c|c}
    \toprule
    Model(number & \multicolumn{3}{|c}{Memory Consumption(MB)} \\
     of parameters) & No Cache & Cache All & CacheInf \\
    \midrule
    % DenseNet(7M) & 30.8 & 159.7 & 53.2\\
    % \hline
    RegNet(54M) & 207.5 & 427.7 & 258.9\\
    \hline
    ConvNeXt(88M) & 337.9 & 603.9 & 369.9\\
    \hline
    VGG19(143M) & 548.1 & 668.7 & 582.8\\
    \hline
    ConvNeXt(197M) & 765.4 & 1152.7 & 789.8 \\
    \bottomrule
    \end{tabular}
    
        \caption{Peak GPU memory consumption of different caching strategy on common visual models. }
        \label{tab:torchvision_mem}
\end{table}

\subsection{Micro-Event}
We first present the micro-events about the real-time inference latency of Kapao of different systems under fluctuating bandwidth in Figure~\ref{fig:micro_e2e}.
And we can learn that CacheInf consistently achieved the lowest inference latency among all the systems and the gain was most significant under lower bandwidth.
Then we fixed the wireless network bandwidth to 48Mb/s and examined different systems's performance at varied cache ratios from a sequence of video images in Figure~\ref{fig:micro_ratio}: at high cache ratios, CacheInf dramatically reduced inference latency compared with other baselines; at low cache ratios, CacheInf degraded to Hybrid-Parallel or even slightly increased inference latency compared with Hybrid-Parallel, and the reason could be the overhead to analyze reusable cache and update cache.
We can also observe when cache ratios fluctuated, the inference latency of CacheInf was more stable than DSCCS-C, which can be attributed to CacheInf's ability to adjust input ratio ($x$) to reduce inference latency.

\begin{figure}[htb]
    \includegraphics[width=0.98\linewidth]{fig/MicroEvent2.png}
    \caption[short]{Kapao: inference latency of different systems at different wireless network bandwidth.}
    \label{fig:micro_e2e}
\end{figure}


\begin{figure}[htb]
    \includegraphics[width=0.98\linewidth]{fig/MicroEvent3.png}
    \caption[short]{Kapao: inference latency of different systems at different cached ratio with fixed wireless network bandwidth.}
    \label{fig:micro_ratio}
\end{figure}

\subsection{Sensitivity}

\begin{table*}[htb]
    \begin{tabular}{c|c|c|c|c}
        \toprule
        \multirow[c]{2}{*}{Model} & \multirow[c]{2}{*}{Statistics} & \multicolumn{3}{c}{Difference Filter Parameter (n)} \\
        \cline{3-5}
        & & 50 & 70 & 130 \\
        \midrule
        \bottomrule

    \end{tabular}
    \caption[accuracy]{How different difference filter parameter (n) for identifying reusable cache affects the inference latency of CacheInf and the accuracy of visual models.}
\end{table*}

\subsection{Sampling Rate of Video Frames}

\begin{table*}[htb]
    \begin{tabular}{c|c|c|c|c}
        \toprule
        \multirow[c]{2}{*}{Model} & \multirow[c]{2}{*}{Statistics} & \multicolumn{3}{|c}{Sampling rate} \\
        \cline{3-5}
        & & 1 & 2 & 4 \\
        \midrule
        \bottomrule

    \end{tabular}
    \caption[sample rates]{How the sampling rate of video frames influence the performance of CacheInf.}
\end{table*}

\subsection{Discussion}