\subsection{Evaluation Settings}

% \begin{figure}[!t]
%     \centering
%     %\vspace{-0.3cm}
%     \subfloat[Four-wheeled robot]{\includegraphics[width=0.48\linewidth]{fig/robot.png}\label{fig:robot}}
%     \hfil
%     \subfloat[Air-ground robot]{\includegraphics[width=0.40\linewidth]{fig/agr_robot.png}\label{fig:agr}}
%     %\vspace{-0.3cm}
%     \caption{The composition of the four-wheeled robot and the air-ground robot used in our evaluation.}
%     % \label{fig:robot}
%     %\vspace{-0.3cm}
% \end{figure}

\myparagraph{Testbed}
We conducted experiments on a four-wheeled robot and an air-ground robot.
Both robots are equipped with a Jetson Xavier NX~\cite{jetsonnx} 8G onboard computer with cuda acceleration capability and a MediaTek MT76x2U USB wireless network interface card for wireless connectivity.
The Jetson Xavier NX is connected to a Leishen N10P LiDAR, an ORBBEC Astra depth camera and an STM32F407VET6 controller via USB serial ports, which are managed and driven using ROS Noetic. 
The GPU server used in our experiments is equipped with an Intel(R) i5 12400f CPU @ 4.40GHz and an NVIDIA GeForce GTX 2080 Ti 11GB GPU, connected to our robot via Wi-Fi 6 over 80MHz channel at 5GHz frequency.

% Tab.~\ref{tab:energydefault} presents the overall on-board energy consumption (excluding motor energy consumption for robot movement) of the robot in various states: inference (model inference with full GPU utilization, including CPU and GPU energy consumption), transmission (communication with the GPU server, including wireless network card energy consumption), and standby (robot has no tasks to execute).
% Notice that different models, due to varying numbers of parameters, exhibit distinct GPU utilization rates and power consumption during inference. 

% \begin{table}[!t]
%     \centering
%     \begin{tabular}{|c|c|c|c|}
%     \hline
%             & inference & transmission & standby \\ \hline
%     Power (W) &     13.35        &       4.25        &    4.04   \\ \hline
%     \end{tabular}
%     \caption{Power consumption (Watt) of our robot in different states.}
%     \label{tab:energydefault}
%     \end{table}

\myparagraph{Workload}
We chose two real-world visual robotic applications as our major workloads: 1. Kapao~\cite{kapao} depicted in Figure~\ref{fig:kapao}, a convolution-neural-network-based (CNN-based) real-time people key point detection applications used to guide our four-wheeled robot to track and follow a walking people;
2. AGRNav~\cite{agrnav} depicted in Figure~\ref{fig:agrnav}, a CNN-based autonomous air-ground robot navigation application that predicts unobserved obstacles by semantic prediction on point clouds and optimizes the navigation trajectory for the air-ground robot.
% We evaluated two typical real-world robotic applications on our testbed: Kapao, a real-time people-tracking application on our four-wheeled robot (Fig~\ref{fig:kapao}), and AGRNav, an autonomous navigation application on our air-ground robot (Fig~\ref{fig:agrnav}). 
% These applications feature different model input and output size patterns: Kapao takes RGB images as input and outputs key points of small data volume. In contrast, AGRNav takes point clouds as input and outputs predicted point clouds and semantics of similar data volume as input, implying that AGRNav needs to transmit more data during distributed inference. 
We also verified CacheInf's performance on a broader range of visual models common to mobile devices: VGGNet~\cite{simonyan2015deep}, ConvNeXt~\cite{woo2023convnext}, ResNet~\cite{targ2016resnet} using their default implementation of torchvision~\cite{noauthor_torchvision_nodate}. 
% And we have verified several models common to mobile devices on a larger scale to further corroborate our observations and findings: DenseNet~\cite{huang2018densely}, VGGNet~\cite{simonyan2015deep}, ConvNeXt~\cite{woo2023convnext}, RegNet~\cite{xu2022regnet}.
% The models' running statistics are listed in Tab.~\ref{tab:all_app}.

\myparagraph{Dataset}
For AGRNav we used the officially available sequence of point clouds input~\cite{agrnav} and for Kapao and the rest of the models from torchvision, we used the DAVIS~\cite{Perazzi2016} dataset which are sequences of video images of people and animals doing different activities captured using hand-held cameras.
% For the rest of the models from torchvision, we used the DAVIS~\cite{Perazzi2016} dataset which are sequences of video images of different objectives captured also using hand-held cameras.

\myparagraph{Experiment Environments}
The experiments across all systems and all workloads were conducted with the robot stationary and the charger plugged in.
We emulate the mobile indoor environment for the experiments by replaying the wireless network bandwidth limits recorded where the robot was moving in our office with desks and separators interfering wireless bandwidth.
We adopt such setting to maintain stable computation performance of the robot across different experiments while ensuring that the setting reflects the limited computation resources and limited and unstable wireless network bandwidth that hinder fast visual model inference on the mobile robot.
% 2. outdoors, where the robot was moving in a garden with trees and bushes interfering with wireless signals and less reflection, resulting in lower bandwidth. 
% The bandwidth fluctuation of each of the environments are shown in Fig.~\ref{fig:bandwidth}.
% We evaluated two real-world environments: indoors (robots move in our laboratory with desks and separators interfering with wireless signals) and outdoors (robots move in our campus garden with trees and bushes interfering with wireless signals, resulting in lower bandwidth). 

\myparagraph{Baselines}
We selected a state-of-the-art (SOTA) cached-based computation reduction method called EVA2~\cite{buckler_eva_2018} and a state-of-the-art (SOTA) computation offloading method, Hybrid-Parallel ~\cite{sun2024hybridparallel} (referred to as HP) as the major baselines.
EVA2 reuses the cached activations when the detected MSE of the input image compared with the cached input image is below a threshold or wholly recomputes otherwise.
HP splits the input image in parallel computes and offloads different splits of the input to accelerate computation offloading.

We integrated EVA2 with HP to enable computation offloading by default for fair comparison, and we alter its MSE threshold to two levels high and low to emulate different choices when EVA2 is applied to moving cameras: high MSE threshold means recomputation will be less likely to be triggered for more activation reusing; low MSE threshold will more frequently trigger recomputation for higher accuracy. 
We also compared its performance with low MSE threshold when computing locally.
These results in three evaluation items: EVA2(H), EVA2(L) and EVA2(L)-L for EVA2 with HP and high MSE threshold, EVA2 with HP and low MSE threshold and local version of EVA2 with low MSE threshold.

We also show the performance of direct local computation of the applications to demonstrate the effectiveness of all these systems.
We refer to CacheInf as Ours in the tables.
Each result in the tables is followed with standard deviation ($\pm n$).

% We selected two SOTA inference acceleration methods as baselines: DSCCS~\cite{liang2023dnn} (referred to as DS), which searches for optimal layer partition strategy of a visual model to offload layers to the GPU server to accelerate inference, and Hybrid-Parallel ~\cite{sun2024hybridparallel} (referred to as HP), which enables parallelization of local computation and offloading by also partitioning within the output of local operators besides layer partitioning to further accelerate inference. 
% We also combined DSCCS with our cache mechanism (referred to as DS-C) to present another perspective about our cache mechanism.

The evaluation questions are as follows:
\begin{itemize}
    \item RQ1: How much does CacheInf benefit real-world robotic applications in terms of inference time, inference accuracy and energy consumption?
    \item RQ2: How does CacheInf perform on more models common to mobile devices?
    \item RQ3: How is the above gain achieved in CacheInf and what affects it?
    \item RQ4: The limitations and potentials of CacheInf.
\end{itemize}

\begin{figure}[!t]
    %\vspace{-0.3cm}
    \centering
    \subfloat[Targeted people]{\includegraphics[width=0.5\linewidth]{fig/people.drawio.png}}
    \subfloat[Robot moving trajectory]{\includegraphics[width=0.5\linewidth]{fig/robot.drawio.png}}
    %\vspace{-0.4cm}
    \caption{A real-time people-tracking robotic application on our robot based on a state-of-the-art human pose estimation visual model, Kapao~\cite{kapao}.}
    \label{fig:kapao}
    %\vspace{-0.4cm}
\end{figure}

\begin{figure}[!t]
    
    %\vspace{-0.3cm}
    \centering
    \subfloat[Indoors]{\includegraphics[width=0.5\linewidth]{fig/agrnav.png}}
    \subfloat[Outdoors]{\includegraphics[width=0.5\linewidth]{fig/agr_outdoors.png}}
    
    %\vspace{-0.4cm}
    \caption{By predicting occlusions in advance, AGRNav~\cite{agrnav} gains an accurate perception of the environment and avoids collisions, resulting in efficient and energy-saving paths.}
    \label{fig:agrnav}
    %\vspace{-0.4cm}
\end{figure}


\subsection{End-to-End Performance on Real-World Applications}

\begin{table*}[htb]
    
    %\vspace{-0.3cm}
    \renewcommand\arraystretch{0.95}
    \centering
\begin{tabular}{ccc|c|c|c|c|c|c}
\toprule
Model(number & Local compu- & \multirow[c]{2}{*}{System} & \multicolumn{2}{|c|}{Transmission time/s} & \multicolumn{2}{|c|}{Inference time/s} & \multicolumn{2}{c}{Percentage(\%)} \\
of parameters)& tation time/s &  & indoors & outdoors & indoors & outdoors & indoors & outdoors \\
\midrule
\multirow[c]{4}{*}{Kapao(77M)} & \multirow[c]{4}{*}{1.01($\pm$0.03)} & DS & 0.21($\pm$0.1) & 0.24($\pm$0.12) & 0.36($\pm$0.2) & 0.40($\pm$0.17) & 58.33 & 60.21 \\
 &  & DS-C & 0.22($\pm$0.14) & 0.25($\pm$0.12) & 0.32($\pm$0.25) & 0.34($\pm$0.18) & 68.75 & 73.53 \\
 &  & HP & 0.24($\pm$0.15) & 0.28($\pm$0.13) & 0.31($\pm$0.14) & 0.34($\pm$0.12) & 77.42 & 82.35 \\
 &  & Ours & 0.16($\pm$0.13) & 0.21($\pm$0.18) & 0.20($\pm$0.16) & 0.24($\pm$0.20) & 80.09 & 87.56 \\
\cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{AGRNav(0.84M)} & \multirow[c]{4}{*}{0.60($\pm$0.04)} & DS & 0.10($\pm$0.05) & 0.15($\pm$0.05) & 0.41($\pm$0.11) & 0.47($\pm$0.12) & 24.39 & 31.91\\
 &  & DS-C & 0.13($\pm$0.07) & 0.16($\pm$0.06) & 0.38($\pm$0.10) & 0.43($\pm$0.13) & 34.21 & 37.21\\
 &  & HP & 0.24($\pm$0.08) & 0.26($\pm$0.07) & 0.30($\pm$0.09) & 0.33($\pm$0.07) & 78.65 & 79.47 \\
 &  & Ours & 0.18($\pm$0.08) & 0.20($\pm$0.08) & 0.21($\pm$0.16) & 0.25($\pm$0.18) & 86.71 & 80.01 \\
\cline{1-9} \cline{2-9}
\bottomrule
\end{tabular}

    \caption{Average transmission time, inference time, percentage that transmission time accounts for of the total inference time of Kapao and AGRNav in different environments with different systems.}
    \label{tab:e2e_time}
    %\vspace{-0.7cm}
\end{table*}

Table~\ref{tab:e2e_time} shows the inference latency and the ratio that transmission time takes up the inference latency and compared with the baselines (we include results of local computation for comparison, referred to as Local), CacheInf reduced inference latency by 35.5\% to 44.4\% indoors and 29.4\% to 40.0\% outdoors for Kapao and 30.0\% to 48.8\% indoors and 24.2\% to 46.8\% outdoors for AGRNav.
Compared with HP, while CacheInf reduced transmission time by 6 to 8 ms, CacheInf further reduced inference latency by 8 to 10 ms, confirming the effectiveness of the acceleration of the used sparse local operators.
CacheInf's highest percentage that transmission time takes up the inference latency across all cases shows that with shrunk transmission data volume with cache enabled and the integration of HP, CacheInf tend to offload computation to the GPU server more often.
This can also be validated by the increased transmission time and reduced inference latency of DSCCS-C compared with DSCCS.

The reduced inference latency of CacheInf leads to reduction of energy consumed per inference by 25.2\% to 34.3\% indoors and 21.2\% to 34.0\% outdoors for Kapao and 27.4\% to 35.7\% indoors and 21.7\% to 39.9\% outdoors for AGRNav, as shown in Table~\ref{tab:e2e_power}, while the runtime power consumption was increased due to higher frequency of inference.

We report the peak GPU memory consumption on the robot under different strategy in Table~\ref{tab:e2e_mem}: CacheInf (includes both the systems of CacheInf and DSCCS-C), No Cache (includes local computation and HP) and Cache All (a naive strategy that caches the output of every layer).
The results show that CacheInf increased peak GPU memory consumption by 64.6\% for Kapao and 58.5\% for AGRNav compared with no cache, which is however 72.2\% and 81.6\% lower than the cases of Cache All, demonstrating the effectiveness of CacheInf's strategy to reduce the number cached operators.


\begin{table}[htb]
    % %\vspace{-0.3cm}
    \renewcommand\arraystretch{0.95}
\tabcolsep=0.095cm
\centering
\begin{tabular}{cc|c|c|c|c}
\toprule
& \multirow[c]{3}{*}{\rotatebox[origin=c]{45}{System}} & \multicolumn{2}{|c}{Power} & \multicolumn{2}{|c}{Energy consumption(J)} \\
& &  \multicolumn{2}{|c}{consumption(W)} & \multicolumn{2}{|c}{per inference}\\
\cline{3-6}
&  & indoors & outdoors & indoors & outdoors \\
\midrule
\midrule
\multirow[c]{5}{*}{\rotatebox[origin=c]{90}{Kapao}} & Local & 9.91($\pm$0.49) & 9.91($\pm$0.49) & 9.79($\pm$0.03) & 9.79($\pm$0.03) \\
 & DS & 6.38($\pm$2.21) & 6.63($\pm$2.38) & 2.30($\pm$0.55) & 2.65($\pm$0.55) \\
 & DS-C & 6.30($\pm$2.15) & 6.53($\pm$2.12) & 2.02($\pm$0.50) & 2.22($\pm$0.53) \\
 & HP & 7.05($\pm$1.63) & 6.94($\pm$0.98) & 2.19($\pm$0.62) & 2.35($\pm$0.42) \\
 & Ours & 7.53($\pm$1.62) & 7.30($\pm$0.96) & 1.51($\pm$0.60) & 1.75($\pm$0.41) \\
\cline{1-6}
\multirow[c]{5}{*}{\rotatebox[origin=c]{90}{AGRNav}} & Local & 8.11($\pm$0.25) & 8.11($\pm$0.25) & 4.86($\pm$0.01) & 4.86($\pm$0.01) \\
 & DS & 6.21($\pm$1.50) & 7.29($\pm$1.55) & 2.55($\pm$0.19) & 3.43($\pm$0.18) \\
 & DS-C & 6.17($\pm$1.56) & 7.00($\pm$1.43) & 2.34($\pm$0.20) & 3.01($\pm$0.20) \\
 & HP & 7.52($\pm$0.51) & 8.04($\pm$0.45) & 2.26($\pm$0.15) & 2.63($\pm$0.15) \\
 & Ours & 7.83($\pm$0.57) & 8.23($\pm$0.56) & 1.64($\pm$0.17) & 2.06($\pm$0.16) \\
\cline{1-6}
\bottomrule
\end{tabular}

    \caption{The power consumption against time (Watt) and energy consumption per inference (Joule) of Kapao and AGRNav different environments with different systems.}
    \label{tab:e2e_power}
    %\vspace{-0.3cm}
\end{table}

\begin{table}[htb]
    %\vspace{-0.2cm}
    \renewcommand\arraystretch{0.95}
\centering
\begin{tabular}{c|c|c|c}
\toprule
Model(number & \multicolumn{3}{|c}{Memory Consumption(MB)} \\
 of parameters) & No Cache & Cache All & CacheInf \\
\midrule
Kapao(77M) & 300.6 & 1782.5 & 494.7\\
\hline
AGRNav(0.84M) & 82.8 & 713.3 & 131.2\\
\bottomrule
\end{tabular}

    \caption{Peak GPU memory consumption of different caching strategy on Kapao and AGRNav. }
    \label{tab:e2e_mem}
    %\vspace{-0.7cm}
\end{table}

\subsection{Performance on Various Common Models}
The above conclusions can be further validated by results of a wider range of visual models in Table~\ref{tab:torchvision_time} and Table~\ref{tab:torchvision_power}.
Across different visual models, CacheInf reduced the inference latency by 13.4\% to 43.6\% indoors and 13.1\% to 45.9\% outdoors, and it results in the reduction in energy consumed per inference to be 11.1\% to 46.7\% indoors and 9.5\% to 42.2\%
compared with the baselines.
Note that although CacheInf's gain is still evident, the lower bound of CacheInf's gain decreased on these models compared with Kapao and AGRNav; the reason could be that these models are less computation-intensive, which can be implied from their shorter time for local computation compared with Kapao and AGRNav.
When inference of a visual models is not computation-intensive, the gain of using sparse local operators in CacheInf will be limited since execution of each local operator will no longer be the bottleneck.
In terms of GPU memory consumption, CacheInf increased GPU memory consumption by 3.2\% to 24.8\% compared with No Cache, while reducing 12.8\% to 39.5\% GPU memory consumption compared with Cache All.

\begin{table*}[htb]

    %\vspace{-0.3cm}
    \renewcommand\arraystretch{0.95}
    \centering
\begin{tabular}{ccc|c|c|c|c|c|c}
\toprule
 Model(number&  Local compu- & \multirow[c]{2}{*}{System} & \multicolumn{2}{|c|}{Transmission time/ms} & \multicolumn{2}{|c|}{Inference time/ms} & \multicolumn{2}{c}{Percentage(\%)} \\
 of parameters) & taion time/ms &  & indoors & outdoors & indoors & outdoors & indoors & outdoors \\
\midrule
% \multirow[c]{4}{*}{DenseNet(7M)} & \multirow[c]{4}{*}{74.5($\pm$18.7)} & DSCCS & 16.2($\pm$40.9) & 20.8($\pm$51.9) & 81.4($\pm$27.2) & 86.6($\pm$27.7) & 19.95 & 24.07 \\
%  &  & DSCCS-C & 20.4($\pm$43.5) & 25.8($\pm$56.9) & 85.5($\pm$27.9) & 89.6($\pm$29.3) & 23.86 & 28.80 \\
%  &  & HP & 53.4($\pm$34.5) & 52.9($\pm$23.9) & 74.5($\pm$85.7) & 55.1($\pm$15.6) & 71.70 & 96.05 \\
%  &  & CacheInf & 56.3($\pm$37.5) & 57.5($\pm$43.5) & 76.3($\pm$90.6) & 78.1($\pm$33.6) & 73.79 & 73.62 \\
% \cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{RegNet(54M)} & \multirow[c]{4}{*}{175.0($\pm$23.6)} & DSCCS & 47.6($\pm$47.8) & 60.5($\pm$54.0) & 77.8($\pm$39.3) & 86.2($\pm$37.9) & 61.22 & 70.22 \\
 &  & DSCCS-C & 50.7($\pm$49.8) & 62.5($\pm$53.6) & 70.8($\pm$33.3) & 79.5($\pm$39.2) & 71.61 & 78.61 \\
 &  & HP & 49.6($\pm$21.7) & 59.9($\pm$23.4) & 55.0($\pm$24.8) & 64.2($\pm$25.2) & 90.18 & 93.34 \\
 &  & CacheInf & 44.2($\pm$27.7) & 48.5($\pm$25.3) & 45.3($\pm$35.0) & 49.2($\pm$37.2) & 97.57 & 98.58 \\
\cline{1-9} \cline{2-9}
% \multirow[c]{4}{*}{ConvNeXt(88M)} & \multirow[c]{4}{*}{160.2($\pm$21.0)} & DSCCS & 46.9($\pm$43.1) & 56.7($\pm$52.1) & 72.4($\pm$35.7) & 84.7($\pm$36.3) & 64.78 & 66.95 \\
%  &  & DSCCS-C & 48.0($\pm$45.0) & 53.2($\pm$50.1) & 56.8($\pm$28.1) & 70.8($\pm$39.0) & 84.51 & 75.14 \\
%  &  & HP & 50.4($\pm$32.2) & 61.9($\pm$34.8) & 53.9($\pm$26.2) & 65.7($\pm$27.7) & 93.51 & 94.23 \\
%  &  & CacheInf & 40.7($\pm$40.0) & 50.7($\pm$40.3) & 46.7($\pm$35.4) & 56.8($\pm$45.0) & 87.15 & 89.26 \\
% \cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{VGG19(143M)} & \multirow[c]{4}{*}{118.0($\pm$18.9)} & DSCCS & 38.9($\pm$47.1) & 41.6($\pm$53.8) & 65.2($\pm$28.1) & 75.5($\pm$27.1) & 59.75 & 55.09 \\
 &  & DSCCS-C & 42.7($\pm$30.2) & 52.0($\pm$50.3) & 53.2($\pm$33.0) & 60.3($\pm$30.9) & 80.26 & 86.24 \\
 &  & HP & 44.8($\pm$20.9) & 51.5($\pm$15.0) & 47.6($\pm$18.1) & 53.6($\pm$14.7) & 94.15 & 96.07 \\
 &  & CacheInf & 37.8($\pm$31.2) & 43.5($\pm$13.2) & 41.1($\pm$20.3) & 46.6($\pm$12.8) & 94.26 & 93.34 \\
\cline{1-9} \cline{2-9}
\multirow[c]{4}{*}{ConvNeXt(197M)} & \multirow[c]{4}{*}{316.7($\pm$31.0)} & DSCCS & 56.0($\pm$36.1) & 67.0($\pm$37.6) & 79.2($\pm$35.9) & 90.6($\pm$35.4) & 70.72 & 73.98 \\
 &  & DSCCS-C & 56.0($\pm$39.0) & 63.0($\pm$30.2) & 64.7($\pm$40.2) & 68.6($\pm$35.0) & 86.55 & 91.84 \\
 &  & HP & 56.4($\pm$34.7) & 66.5($\pm$33.7) & 59.7($\pm$26.6) & 68.0($\pm$26.6) & 94.43 & 97.88 \\
 &  & CacheInf & 40.4($\pm$37.8) & 46.9($\pm$40.0) & 44.7($\pm$33.3) & 49.0($\pm$30.8) & 90.38 & 95.71 \\
\cline{1-9} \cline{2-9}
\bottomrule
\end{tabular}
    \caption{Average transmission time, inference time, percentage that transmission time accounts for of the total inference time of common visual models in different environments with different systems. }
    \label{tab:torchvision_time}
    %\vspace{-0.3cm}
\end{table*}

\begin{table}[htb]

    %\vspace{-0.3cm}
    \renewcommand\arraystretch{0.95}
\centering
\tabcolsep=0.12cm
\begin{tabular}{cc|c|c|c|c}
\toprule
 & \multirow[c]{3}{*}{\rotatebox[origin=c]{45}{System}} & \multicolumn{2}{|c}{Power} & \multicolumn{2}{|c}{Energy consumption(J)} \\
& &  \multicolumn{2}{|c}{consumption(W)} & \multicolumn{2}{|c}{per inference}\\
\cline{3-6}
 &  & indoors & outdoors & indoors & outdoors \\
\midrule
% \multirow[c]{5}{*}{DenseNet(7M)} & Local & 8.2($\pm$0.27) & 8.2($\pm$0.27) & 0.46($\pm$0.04) & 0.46($\pm$0.04) \\
%  & DS & 6.91($\pm$0.45) & 6.86($\pm$0.46) & 0.56($\pm$0.04) & 0.59($\pm$0.04) \\
%  & DS-C & 7.01($\pm$0.43) & 6.96($\pm$0.43) & 0.60($\pm$0.07) & 0.52($\pm$0.06) \\
%  & HP & 5.36($\pm$0.79) & 5.79($\pm$0.24) & 0.4($\pm$0.06) & 0.32($\pm$0.01) \\
%  & Ours & 6.01($\pm$0.92) & 6.31($\pm$0.56) & 0.46($\pm$0.12) & 0.49($\pm$0.01) \\
% \cline{1-6}
\multirow[c]{5}{*}{\rotatebox[origin=c]{90}{RegNet}} & Local & 9.0($\pm$0.3) & 9.0($\pm$0.3) & 1.37($\pm$0.02) & 1.37($\pm$0.02) \\
& DS & 5.84($\pm$1.79) & 5.36($\pm$1.34) & 0.45($\pm$0.14) & 0.46($\pm$0.12) \\
 & DS-C & 6.04($\pm$1.88) & 5.96($\pm$1.45) & 0.43($\pm$0.16) & 0.47($\pm$0.19) \\
 & HP & 5.24($\pm$1.43) & 5.28($\pm$1.52) & 0.29($\pm$0.08) & 0.34($\pm$0.1) \\
 & Ours & 5.20($\pm$1.51) & 5.23($\pm$1.77) & 0.24($\pm$0.08) & 0.26($\pm$0.09) \\
\cline{1-6}
% \multirow[c]{5}{*}{ConvNeXt(88M)} & Local & 9.7($\pm$0.34) & 9.7($\pm$0.34) & 1.34($\pm$0.02) & 1.34($\pm$0.02) \\
% & DS & 6.01($\pm$0.27) & 5.71($\pm$1.56) & 0.43($\pm$0.05) & 0.48($\pm$0.13) \\
% & DS-C & 6.20($\pm$0.33) & 5.91($\pm$0.21) & 0.35($\pm$0.17) & 0.42($\pm$0.25) \\
%  & HP & 6.68($\pm$1.23) & 6.68($\pm$1.21) & 0.36($\pm$0.07) & 0.44($\pm$0.08) \\
%  & Ours & 6.70($\pm$0.55) & 6.63($\pm$0.26) & 0.31($\pm$0.07) & 0.38($\pm$0.08) \\
% \cline{1-6}
\multirow[c]{5}{*}{\rotatebox[origin=c]{90}{VGG19}} & Local & 9.78($\pm$0.34) & 9.78($\pm$0.34) & 0.95($\pm$0.02) & 0.95($\pm$0.02) \\
& DS & 6.58($\pm$2.14) & 6.93($\pm$2.35) & 0.43($\pm$0.14) & 0.52($\pm$0.18) \\
 & DS-C & 6.82($\pm$2.10) & 7.23($\pm$2.45) & 0.36($\pm$0.18) & 0.43($\pm$0.30) \\
 & HP & 6.51($\pm$1.74) & 7.32($\pm$1.52) & 0.31($\pm$0.08) & 0.39($\pm$0.08) \\
 & Ours & 6.70($\pm$1.88) & 7.22($\pm$1.36) & 0.27($\pm$0.10) & 0.34($\pm$0.09) \\
\cline{1-6}
\multirow[c]{5}{*}{\rotatebox[origin=c]{90}{ConvNeXt}} & Local & 9.92($\pm$0.38) & 9.92($\pm$0.38) & 3.12($\pm$0.03) & 3.12($\pm$0.03) \\
& DS & 5.06($\pm$0.31) & 5.02($\pm$0.37) & 0.4($\pm$0.02) & 0.45($\pm$0.03) \\
 & DS-C & 4.86($\pm$0.44) & 4.99($\pm$0.39) & 0.31($\pm$0.05) & 0.34($\pm$0.09) \\
 & HP & 4.57($\pm$0.23) & 4.54($\pm$0.25) & 0.27($\pm$0.01) & 0.31($\pm$0.02) \\
 & Ours & 5.26($\pm$0.40) & 5.39($\pm$0.27) & 0.24($\pm$0.05) & 0.26($\pm$0.04) \\
\cline{1-6}
\bottomrule
\end{tabular}
    \caption{The power consumption against time (Watt) and energy consumption per inference (Joule) of common visual models in different environments with different systems.}
    \label{tab:torchvision_power}
    %\vspace{-0.5cm}
\end{table}

\begin{table}[htb]
    
    %\vspace{-0.3cm}
    \renewcommand\arraystretch{0.95}
    \centering
    \begin{tabular}{c|c|c|c}
    \toprule
    Model(number & \multicolumn{3}{|c}{Memory Consumption(MB)} \\
     of parameters) & No Cache & Cache All & CacheInf \\
    \midrule
    % DenseNet(7M) & 30.8 & 159.7 & 53.2\\
    % \hline
    RegNet(54M) & 207.5 & 427.7 & 258.9\\
    \hline
    % ConvNeXt(88M) & 337.9 & 603.9 & 369.9\\
    % \hline
    VGG19(143M) & 548.1 & 668.7 & 582.8\\
    \hline
    ConvNeXt(197M) & 765.4 & 1152.7 & 789.8 \\
    \bottomrule
    \end{tabular}
    
        \caption{Peak GPU memory consumption of different caching strategy on common visual models. }
        \label{tab:torchvision_mem}
    %\vspace{-0.8cm}
\end{table}

\subsection{Micro-Event}
We first present the micro-events about the real-time inference latency of Kapao of different systems under fluctuating bandwidth in Figure~\ref{fig:micro_e2e}.
And we can learn that CacheInf consistently achieved the lowest inference latency among all the systems and the gain was most significant under lower bandwidth.
Then we fixed the wireless network bandwidth to 48Mb/s and examined different systems's performance at varied cache ratios from a sequence of video images in Figure~\ref{fig:micro_ratio}: at high cache ratios, CacheInf dramatically reduced inference latency compared with other baselines; at low cache ratios, CacheInf degraded to Hybrid-Parallel or even slightly increased inference latency compared with Hybrid-Parallel, and the reason could be the overhead to analyze reusable cache and update cache.
We can also observe when cache ratios fluctuated, the inference latency of CacheInf was more stable than DSCCS-C, which can be attributed to CacheInf's ability to adjust input ratio ($x$) to reduce inference latency.

\begin{figure}[htb]
    %\vspace{-0.3cm}
    \includegraphics[width=\linewidth]{fig/MicroEvent2.png}
    %\vspace{-0.3cm}
    \caption[short]{Kapao: inference latency of different systems at different wireless network bandwidth.}
    \label{fig:micro_e2e}
    %\vspace{-0.3cm}
\end{figure}


\begin{figure}[htb]
    %\vspace{-0.3cm}
    \includegraphics[width=\linewidth]{fig/MicroEvent3.png}
    %\vspace{-0.3cm}
    \caption[short]{Kapao: inference latency of different systems at different cached ratio with fixed wireless network bandwidth.}
    \label{fig:micro_ratio}
    %\vspace{-0.3cm}
\end{figure}

\subsection{Sensitivity}

\begin{table}[htb]
    %\vspace{-0.3cm}
    \begin{tabular}{c|c|c|c|c}
        \toprule
        \multirow[c]{2}{*}{Model} & \multirow[c]{2}{*}{Statistics} & \multicolumn{3}{c}{N} \\
        \cline{3-5}
        & & 0.1 & 0.2 & 0.3 \\
        \midrule
        \multirow[c]{2}{*}{Kapao} & Inference Latency/s & 0.20 & 0.18 & 0.17\\
        \cline{2-5}
            & accuracy (AP) & 75.8 & 74.6 & 72.5 \\
        \hline
        \multirow[c]{2}{*}{AGRNav} & Inference Latency/s & 0.21 & 0.19 & 0.17 \\
        \cline{2-5}
            & accuracy (F1) & 98.9 & 98.5 & 98.4\\
        \hline
        \multirow[c]{2}{*}{ConvNeXt} & Inference Latency/ms & 44.7 & 38.6 & 34.8\\
        \cline{2-5}
            & accuracy (Acc@1) & 100.0 & 100.0 & 99.2 \\
        \bottomrule
    \end{tabular}
    \caption[accuracy]{How different difference filtering threshold (N) for identifying reusable cache affects the inference latency of CacheInf indoors and the accuracy of visual models.}
    %\vspace{-0.8cm}
    \label{tab:sensitvity}
\end{table}

With higher difference filtering threshold ($N$), CacheInf will mark cached computation result for areas with more difference as reusable, and we present its influence on the accuracy performance of the selected representative visual models in Table~\ref{tab:sensitvity}.
We used the output of the same model with the same input under local computation as the groundtruth to compute accuracy.
Each model has a different accuracy metric: AP stands for average precision for people detection for Kapao; F1 examines the portion of points in a point cloud that is close to the groundtruth; Acc@1 is the percentage of the predicted classification results matching with the groundtruth.
From Table~\ref{tab:sensitvity} we can learn that loosening the constraint of cache identification by increasing $N$ slightly decreased accuracy of visual models, with the advantage of further reduced inference latency.
And for visual models with pixel level output (e.g., predicted people pose of Kapao and predicted point cloud of AGRNav), such influence will be perceptible, while it does not significantly affect the accuracy of the visual models with comprehensive output (e.g., classification results of ConvNeXt).





% \begin{table*}[htb]
%     \begin{tabular}{c|c|c|c|c}
%         \toprule
%         \multirow[c]{2}{*}{Model} & \multirow[c]{2}{*}{Statistics} & \multicolumn{3}{|c}{Sampling rate} \\
%         \cline{3-5}
%         & & 1 & 2 & 4 \\
%         \midrule
%         \bottomrule

%     \end{tabular}
%     \caption[sample rates]{How the sampling rate of video frames influence the performance of CacheInf.}
% \end{table*}

\subsection{Discussion}
From the above results we can learn that CacheInf is fundamentally trading-off between GPU memory with inference latency, just as systems in other domains with cache enabled.
Since the resulting increased GPU memory consumption may be unfavorable for devices with tight memory budget, adjusting such trade-off to further reduce extra GPU memory consumption to fit in these devices will be our future work.
Another limitation of CacheInf is that it relies on continuity of input and thus is unsuitable for scenarios where the perspective of the robot changes dramatically.