Visual information is vital for various robotic tasks deployed on mobile edge devices (mobile robots), such as navigation , manipulation , and human-robot interaction ; and as a major visual information processing method, fast, accurate and energy-efficient visual model inference is important for the robotic tasks to timely respond to environment changes. Unfortunately, the mobile robot typically has limited computational power and limited and unstable wireless network bandwidth , which slow down both local computation and naive offloading of computation to GPU servers. Thus the mobile robot often suffers the problem of slow visual model inference that interferes the robotic task performance (e.g., 1.xx seconds in human pose estimation  or 2.xx seconds in surrounding occlusion prediction ) and the prolonged inference latency naturally increases energy consumption each inference.
To address this problem, we seek enlightenment from previous work targeting fixed edge devices that decrease the amount of computation required in visual model inference with the caching mechanism . They are based on the fact that the visual models extensively use operators (e.g., convolution) whose computation results (i.e. activations) are spatially correlated to the input image: the value of each pixel on the activations is dominated by a block of the input image (i.e., receptive field) determined by the model architecture . Given a continuous stream of images, they either 1. recognize the movements of the receptive fields and interpolate (reuse) the cached activations of an operator accordingly to skip the activation computation of this and the previous operators, or 2. in cases of movements recognition failure, execute full local computation (recompute) on the input image to get the latest activations.
While introducing such mechanism to reuse the cached previous activations to visual model inference on mobile robots seems able to skip local activation computation and reduce transmission data volume in computation offloading (since only recognized movements needs to be transmitted), it faces a dilemma between inference accuracy and inference speed on mobile robots which feature frequent camera perspective movements . Changes in camera perspective typically bring new scenes or changed occlusion into the images, which cannot be covered by receptive field movements (recognition failure). To cope with this situation, the above methods can only either ignore certain recognition failure at the cost of severely degraded inference accuracy (25.89% lower in ), or frequently execute full local computation or full transmission of the input images during computation offloading, sacrificing inference speed.
The key reason to their dilemma is that while reusing or recomputing the activations optimizes the inference speed or accuracy and sacrifices the opposite, there lacks a tradeoff method between inference accuracy and speed. When continuously reusing activations computed with a reference frame, the difference between the new frames and the reference frame accumulates and causes either degraded accuracy or triggering of full recomputation to eliminate error in activations; but with the ability to tradeoff between accuracy and inference speed (for example, partially reusing and partially recomputing the activations), we can selectively compute on the blocks with most differences while reusing the computation results of the others, mitigating the accumulation of error while accelerating both local computation and computation offloading via caching.
To bridge this gap, in this paper, we propose CacheInf, a high-performance collaborative edge-cloud cache system for efficient robotic visual model. Given a continuous stream of visual input in a robotic visual task, CacheInf selectively reuses a portion of the cached activations of a reference frame while recomputes the rest based on statistical metrics such as the mean square error of pixels in corresponding receptive fields between the reference frame and the current frame. In this way, the amount of required computation is minimized which accelerates both local computation and computation offloading and the error accumulation in activations is mitigated, achieving the optimal tradeoff between inference accuracy and inference speed for collaborative edge-cloud visual model inference on the mobile robot.
The first challenge of the the design of CacheInf is to minimize computation using cache mechanism without compromising the inference accuracy. While prioritizing recomputation on receptive fields with most difference is intuitively effective, we also notice under the same level of difference between consecutive images, if we choose to cache the activations computed by an operator closer to the input image (former), the portion of activations needed to be recomputed would be reduced. This is because the former operators have smaller receptive fields compared with the latter ones whose activations are less affected by difference across images, but there are more operators after the cached operator whose computation are not skipped. Based on these observations, we design a mechanism in CacheInf to both selectively and adaptively choose the operator to cache its computed activations and the portion of activations to be recomputed according to the levels of difference across input images detected and the amount of possible computation reduction, achieving maximal computation reduction while maintaining high inference accuracy.
The second challenge is to minimize the overall inference latency considering the interaction between the mobile robot and the GPU server. One major obstacle is that switching between local computation and computation offloading under the caching mechanism (e.g., when wireless network bandwidth changes) typically requires costly one full local recomputation or full transmission of the input image, and such cost hinders an eager scheduling method from adopting such switching to gain further overall acceleration. To overcome this issue, we schedule between local computation and computation offloading by looking ahead several steps with the predicted future wireless network bandwidth and possible computation reduction based on previous records, to minimize the overall inference latency.
We implement CacheInf based on python and pytorch integrated with self-implemented C++ cuda extensions. For the tail cases where we need to fully offload an input image to the GPU server, we integrate a state-of-the-art offloading method called Hybrid-Parallel (HP)  which mitigates the caused latency. Our baselines include HP, a state-of-the-art cache-based computation reduction methods called EVA2 , EVA2 integrated with HP and local computation. We evaluated CacheInf on a four-wheel robot equipped with a Jetson NX Xavier  that is capable of computing locally with its low-power-consumption GPU. The offloading GPU server is a PC equipped with an Nvidia 2080ti GPU. Our datasets include the standard datasets of video frames of DAVIS  and CAD  each captured by a handheld camera and our self-captured video frames using sensors on our robot. Extensive evaluation over various visual models  and wireless network bandwidth circumstances shows that:
• CacheInf is fast and accurate. Among the baselines, CacheInf reduced the end-to-end inference time by 13.1% to 48.8% with only 0.21% to 0.96% accuracy reduction.
• CacheInf saves energy. Among the baselines, CacheInf reduced the average energy consumed to complete inference on each image by 9.5% to 39.9%.
• CacheInf is robust. Under different level of difference across consecutive input images, the advantages of CacheInf remained.
The contribution of this paper is twofold: 1. a new caching mechanism for visual model inference on mobile devices which selectively reuse and recompute fractions of cached activations to best tradeoff between inference accuracy and computation reduction; 2. a scheduling mechanism designed to optimize the overall inference latency considering the interaction between the edge (the mobile robot) and the cloud (the GPU server to offload computation to) And the resulting system, CacheInf, optimally reduces visual model inference latency and energy consumption on the mobile robot. The accelerated visual model inference and the reduced power consumption will make real-world robots more performant on various robotic tasks and nurture more visual models to be deployed in real-world robots.